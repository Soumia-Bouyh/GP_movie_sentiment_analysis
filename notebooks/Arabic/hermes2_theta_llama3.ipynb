{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMg68HHR_Vpm",
        "outputId": "a39c8be6-50c0-406a-eb69-3afca5bd4a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgABS1Ml_k-b",
        "outputId": "d32a230e-99e1-4d66-ac68-2f2212534191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.77.tar.gz (50.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.77-cp310-cp310-linux_x86_64.whl size=3695621 sha256=f1eaae5cf38acb53cc83bfa9f54316b2bbe4106b38694c752ffea0ad304dbd84\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/55/a1/6d6c2ef6fed3ef054b4170d8bcd05a09e6dc971db7fad955ff\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.77\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7AnMOjvANrs",
        "outputId": "d4906903-00dd-477a-d450-feedce3adac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4gV_kgU_moj"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201,
          "referenced_widgets": [
            "d7954c5315d2426796aa1058712af85b",
            "4e02be5bf7b842819578df9d84c45070",
            "bd3c9db1f7b34848ba32171947ee6681",
            "f099d344f53347d39ab2dbfaf300b380",
            "b081098f166a4daa8eee0cca71597e10",
            "853e05b268624dbaa4f999dd51694cb5",
            "9f6c7c461f014c7a8eb387143897dc70",
            "d3b7e4b99cad4a00924200df2d35256c",
            "c9467fbef73741599d3148a8eb856403",
            "f90ce1e8169d47f495ae0d225f1db003",
            "f8c5e7e8b50942a5aff2dff4d8d1c7d5"
          ]
        },
        "id": "3sZAb5cKA4UF",
        "outputId": "53395e2d-2785-4c94-8a48-000f46e0b11c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7954c5315d2426796aa1058712af85b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF\"\n",
        "model_file = \"Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\" # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\n",
        "model_path = hf_hub_download(model_name, filename=model_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8GmbkKbCC0i",
        "outputId": "1fcbe65d-ee55-4914-f014-e8d98a14ebb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B-GGUF/snapshots/ded6bdf5c60c7211eb8cb5cec6c91fa358b509b7/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Hermes-2-Pro-Llama-3-Instruct-Merged-DPO\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128003\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7998 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
            "llm_load_print_meta: general.name     = Hermes-2-Pro-Llama-3-Instruct-Merged-DPO\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128003 '<|im_end|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128003 '<|im_end|>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
            "........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 16000\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  2000.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2000.00 MiB, K (f16): 1000.00 MiB, V (f16): 1000.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =  1063.26 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128003', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Hermes-2-Pro-Llama-3-Instruct-Merged-DPO', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '15', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|begin_of_text|>\n"
          ]
        }
      ],
      "source": [
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=16000,  # Context length to use\n",
        "    n_threads=32,  # Number of CPU threads to use\n",
        "    n_gpu_layers=0  # Number of model layers to offload to GPU\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhfoaYMHCIoO"
      },
      "outputs": [],
      "source": [
        "## Generation kwargs\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\": 20000,\n",
        "    \"stop\": [\"</s>\"],\n",
        "    \"echo\": False,  # Echo the prompt in the output\n",
        "    \"top_k\": 1  # This is essentially greedy decoding\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU87nwmFCL2l",
        "outputId": "889fe821-7fe6-4cc8-e977-763dd1e62500"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    4564.19 ms\n",
            "llama_print_timings:      sample time =    8772.92 ms /  3013 runs   (    2.91 ms per token,   343.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4563.71 ms /     7 tokens (  651.96 ms per token,     1.53 tokens per second)\n",
            "llama_print_timings:        eval time = 2153127.82 ms /  3012 runs   (  714.85 ms per token,     1.40 tokens per second)\n",
            "llama_print_timings:       total time = 2186921.44 ms /  3019 tokens\n"
          ]
        }
      ],
      "source": [
        "## Run inference\n",
        "prompt = \"The meaning of life is \"\n",
        "res = llm(prompt, **generation_kwargs)  # Res is a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDlukjHLCNkJ",
        "outputId": "677c6773-00a0-40ca-a4ee-f72a9b1922f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42, or so says Douglas Adams in his science fiction series \"The Hitchhiker's Guide to the Galaxy.\" But what does that really mean? Is it just a humorous way to answer a profound question, or is there something deeper at play?\n",
            "In this episode of Philosophy Bites, David Edmonds and Nigel Warburton discuss the meaning of life with philosopher Simon Blackburn. They explore whether there can be a single answer to the question, and whether it's even possible for us to know what that answer might be.\n",
            "Blackburn suggests that the search for the meaning of life is often driven by our fear of death, and that we may never find a definitive answer because the concept of \"meaning\" itself is subjective. He also argues that there are many different ways to live a meaningful life, depending on one's values and beliefs.\n",
            "Ultimately, Blackburn concludes that while we may not be able to definitively answer the question of what the meaning of life is, we can still strive to create our own personal meanings through our actions and relationships.\n",
            "So, is 42 really the answer? Well, as Adams himself said, \"The answer to the ultimate question of life, the universe and everything is simply '42', which is obviously absurd.\" But perhaps that's the point - there may not be a simple or definitive answer to such a profound and complex question. Instead, we must find our own meaning in the journey itself.\n",
            "Listen to Philosophy Bites: The Meaning of Life with Simon Blackburn\n",
            "David Edmonds: Welcome to Philosophy Bites, I'm David Edmonds. Today's episode is all about the meaning of life. Joining me is Nigel Warburton and our guest is philosopher Simon Blackburn.\n",
            "Nigel Warburton: That's right, David. So, Simon, let's start with a question that has puzzled people for centuries: what is the meaning of life?\n",
            "Simon Blackburn: Well, it's a very difficult question to answer because there are so many different ways in which we might approach it. Some people think that the meaning of life can be found in some kind of ultimate truth or purpose, something that gives our lives significance and direction. Others think that the meaning of life is more subjective, that it's up to each individual to create their own sense of purpose and meaning.\n",
            "David Edmonds: So, do you think there is a single answer to this question?\n",
            "Simon Blackburn: I don't think so. I think that the search for the meaning of life is often driven by our fear of death, and the desire to leave some kind of lasting legacy or impact on the world. But ultimately, I think that each person has to find their own way of giving their life meaning.\n",
            "Nigel Warburton: That's interesting. So, do you think it's possible for us to know what the answer is?\n",
            "Simon Blackburn: Well, I think that the concept of \"meaning\" itself is subjective and can vary from person to person. What gives one person's life meaning might not be the same for another. And even if we could agree on some kind of ultimate truth or purpose, it would still be up to each individual to decide how to live their life in accordance with that purpose.\n",
            "David Edmonds: So, what do you think is the most common mistake people make when thinking about the meaning of life?\n",
            "Simon Blackburn: I think one common mistake is to assume that there must be some kind of single answer or truth that can give our lives meaning. But I think that's a mistake because it assumes that there is only one way to live a meaningful life, and that's just not true.\n",
            "Nigel Warburton: That's a good point. So, what do you think is the best way for people to approach this question?\n",
            "Simon Blackburn: Well, I think that each person has to find their own way of giving their life meaning. It might involve pursuing certain values or goals, developing relationships with others, or engaging in activities that bring them joy and fulfillment. Ultimately, it's up to each individual to decide what gives their life meaning.\n",
            "David Edmonds: So, do you think there is any value in trying to answer this question?\n",
            "Simon Blackburn: Yes, I think that the search for the meaning of life can be a valuable exercise because it forces us to reflect on our values and priorities. It can help us to clarify what's important to us and to make decisions about how we want to live our lives.\n",
            "Nigel Warburton: That's a good point. Simon, thank you very much for joining us today.\n",
            "Simon Blackburn: Thank you for having me.\n",
            "David Edmonds: And that's it for this episode of Philosophy Bites. Thanks for listening, and join us next time when we'll be discussing another fascinating topic in philosophy. [Outro music plays] ... [End of audio]\n",
            "The meaning of life is 42, or so says Douglas Adams in his science fiction series \"The Hitchhiker's Guide to the Galaxy.\" But what does that really mean? Is it just a humorous way to answer a profound question, or is there something deeper at play?\n",
            "In this episode of Philosophy Bites, David Edmonds and Nigel Warburton discuss the meaning of life with philosopher Simon Blackburn. They explore whether there can be a single answer to the question, and whether it's even possible for us to know what that answer might be.\n",
            "Blackburn suggests that the search for the meaning of life is often driven by our fear of death, and that we may never find a definitive answer because the concept of \"meaning\" itself is subjective. He also argues that there are many different ways to live a meaningful life, depending on one's values and beliefs.\n",
            "Ultimately, Blackburn concludes that while we may not be able to definitively answer the question of what the meaning of life is, we can still strive to create our own personal meanings through our actions and relationships.\n",
            "So, is 42 really the answer? Well, as Adams himself said, \"The answer to the ultimate question of life, the universe and everything is simply '42', which is obviously absurd.\" But perhaps that's the point - there may not be a simple or definitive answer to such a profound and complex question. Instead, we must find our own meaning in the journey itself.\n",
            "Listen to Philosophy Bites: The Meaning of Life with Simon Blackburn\n",
            "David Edmonds: Welcome to Philosophy Bites, I'm David Edmonds. Today's episode is all about the meaning of life. Joining me is Nigel Warburton and our guest is philosopher Simon Blackburn.\n",
            "Nigel Warburton: That's right, David. So, Simon, let's start with a question that has puzzled people for centuries: what is the meaning of life?\n",
            "Simon Blackburn: Well, it's a very difficult question to answer because there are so many different ways in which we might approach it. Some people think that the meaning of life can be found in some kind of ultimate truth or purpose, something that gives our lives significance and direction. Others think that the meaning of life is more subjective, that it's up to each individual to create their own sense of purpose and meaning.\n",
            "David Edmonds: So, do you think there is a single answer to this question?\n",
            "Simon Blackburn: I don't think so. I think that the search for the meaning of life is often driven by our fear of death, and the desire to leave some kind of lasting legacy or impact on the world. But ultimately, I think that each person has to find their own way of giving their life meaning.\n",
            "Nigel Warburton: That's interesting. So, do you think it's possible for us to know what the answer is?\n",
            "Simon Blackburn: Well, I think that the concept of \"meaning\" itself is subjective and can vary from person to person. What gives one person's life meaning might not be the same for another. And even if we could agree on some kind of ultimate truth or purpose, it would still be up to each individual to decide how to live their life in accordance with that purpose.\n",
            "David Edmonds: So, what do you think is the most common mistake people make when thinking about the meaning of life?\n",
            "Simon Blackburn: I think one common mistake is to assume that there must be some kind of single answer or truth that can give our lives meaning. But I think that's a mistake because it assumes that there is only one way to live a meaningful life, and that's just not true.\n",
            "Nigel Warburton: That's a good point. So, what do you think is the best way for people to approach this question?\n",
            "Simon Blackburn: Well, I think that each person has to find their own way of giving their life meaning. It might involve pursuing certain values or goals, developing relationships with others, or engaging in activities that bring them joy and fulfillment. Ultimately, it's up to each individual to decide what gives their life meaning.\n",
            "David Edmonds: So, do you think there is any value in trying to answer this question?\n",
            "Simon Blackburn: Yes, I think that the search for the meaning of life can be a valuable exercise because it forces us to reflect on our values and priorities. It can help us to clarify what's important to us and to make decisions about how we want to live our lives.\n",
            "Nigel Warburton: That's a good point. Simon, thank you very much for joining us today.\n",
            "Simon Blackburn: Thank you for having me.\n",
            "David Edmonds: And that's it for this episode of Philosophy Bites. Thanks for listening, and join us next time when we'll be discussing another fascinating topic in philosophy. [Outro music plays] ... [End of audio]://www.youtube.com/watch?v=VHJLw8Q2q4E&list=PLu0K1Gy3-IszN6PfZ5WUk9Tm7RvCtjxX-&index=11&t=0s\n",
            "The meaning of life is 42, or so says Douglas Adams in his science fiction series \"The Hitchhiker's Guide to the Galaxy.\" But what does that really mean? Is it just a humorous way to answer a profound question, or is there something deeper at play?\n",
            "In this episode of Philosophy Bites, David Edmonds and Nigel Warburton discuss the meaning of life with philosopher Simon Blackburn. They explore whether there can be a single answer to the question, and whether it's even possible for us to know what that answer might be.\n",
            "Blackburn suggests that the search for the meaning of life is often driven by our fear of death, and that we may never find a definitive answer because the concept of \"meaning\" itself is subjective. He also argues that there are many different ways to live a meaningful life, depending on one's values and beliefs.\n",
            "Ultimately, Blackburn concludes that while we may not be able to definitively answer the question of what the meaning of life is, we can still strive to create our own personal meanings through our actions and relationships.\n",
            "So, is 42 really the answer? Well, as Adams himself said, \"The answer to the ultimate question of life, the universe and everything is simply '42', which is obviously absurd.\" But perhaps that's the point - there may not be a simple or definitive answer to such a profound and complex question. Instead, we must find our own meaning in the journey itself.\n",
            "Listen to Philosophy Bites: The Meaning of Life with Simon Blackburn\n",
            "David Edmonds: Welcome to Philosophy Bites, I'm David Edmonds. Today's episode is all about the meaning of life. Joining me is Nigel Warburton and our guest is philosopher Simon Blackburn.\n",
            "Nigel Warburton: That's right, David. So, Simon, let's start with a question that has puzzled people for centuries: what is the meaning of life?\n",
            "Simon Blackburn: Well, it's a very difficult question to answer because there are so many different ways in which we might approach it. Some people think that the meaning of life can be found in some kind of ultimate truth or purpose, something that gives our lives significance and direction. Others think that the meaning of life is more subjective, that it's up to each individual to create their own sense of purpose and meaning.\n",
            "David Edmonds: So, do you think there is a single answer to this question?\n",
            "Simon Blackburn: I don't think so. I think that the search for the meaning of life is often driven by our fear of death, and the desire to leave some kind of lasting legacy or impact on the world. But ultimately, I think that each person has to find their own way of giving their life meaning.\n",
            "Nigel Warburton: That's interesting. So, do you think it's possible for us to know what the answer is?\n",
            "Simon Blackburn: Well, I think that the concept of \"meaning\" itself is subjective and can vary from person to person. What gives one person's life meaning might not be the same for another. And even if we could agree on some kind of ultimate truth or purpose, it would still be up to each individual to decide how to live their life in accordance with that purpose.\n",
            "David Edmonds: So, what do you think is the most common mistake people make when thinking about the meaning of life?\n",
            "Simon Blackburn: I think one common mistake is to assume that there must be some kind of single answer or truth that can give our lives meaning. But I think that's a mistake because it assumes that there is only one way to live a meaningful life, and that's just not true.\n",
            "Nigel Warburton: That's a good point. So, what do you think is the best way for people to approach this question?\n",
            "Simon Blackburn: Well, I think that each person has to find their own way of giving their life meaning. It might involve pursuing certain values or goals, developing relationships with others, or engaging in activities that bring them joy and fulfillment. Ultimately, it's up to each individual to decide what gives their life meaning.\n",
            "David Edmonds: So, do you think there is any value in trying to answer this question?\n",
            "Simon Blackburn: Yes, I think that the search for the meaning of life can be a valuable exercise because it forces us to reflect on our values and priorities. It can help us to clarify what's important to us and to make decisions about how we want to live our lives.\n",
            "Nigel Warburton: That's a good point. Simon, thank you very much for joining us today.\n",
            "Simon Blackburn: Thank you for having me.\n",
            "David Edmonds: And that's it for this episode of Philosophy Bites. Thanks for listening, and join us next time when we'll be discussing another fascinating topic in philosophy. [Outro music plays] ... [End of audio]\n"
          ]
        }
      ],
      "source": [
        "print(res[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC9zqupnM42G",
        "outputId": "53dfba5d-db1d-4d01-d3a2-bbff3af77f33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT2uy3cxM8L3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "## Load your TSV dataset\n",
        "df = pd.read_csv('merged_reviews_finalDS.tsv', delimiter='\\t')  # Replace with your dataset path\n",
        "reviews = df['text'].tolist()\n",
        "labels = df['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpcamt5NXQJq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "## Function to classify review using the model\n",
        "def classify_review(review, model, generation_kwargs):\n",
        "    prompt = f\"Classify the sentiment of this review as positive, neutral, or negative: '{review}'\"\n",
        "    response = model(prompt, **generation_kwargs)\n",
        "    text = response[\"choices\"][0][\"text\"].lower()\n",
        "    if 'positive' in text:\n",
        "        return 1\n",
        "    elif 'negative' in text:\n",
        "        return -1\n",
        "    elif 'neutral' in text:\n",
        "        return 0\n",
        "    else:\n",
        "        return None  # In case the response is ambiguous\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUsLhiWhXTnv",
        "outputId": "c77283a1-2452-47b8-ec31-c55b0f8f6843"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Generate predictions for all reviews\n",
        "predictions = [classify_review(review, llm, generation_kwargs) for review in reviews]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiXTAGjHXVpd"
      },
      "outputs": [],
      "source": [
        "## Filter out None values (ambiguous classifications)\n",
        "filtered_reviews = [reviews[i] for i in range(len(predictions)) if predictions[i] is not None]\n",
        "filtered_labels = [labels[i] for i in range(len(predictions)) if predictions[i] is not None]\n",
        "filtered_predictions = [predictions[i] for i in range(len(predictions)) if predictions[i] is not None]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJzt9ueDXXMj"
      },
      "outputs": [],
      "source": [
        "## Calculate accuracy\n",
        "accuracy = accuracy_score(filtered_labels, filtered_predictions)\n",
        "\n",
        "## Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(filtered_labels, filtered_predictions)\n",
        "\n",
        "## Generate classification report\n",
        "class_report = classification_report(filtered_labels, filtered_predictions, target_names=['negative', 'neutral', 'positive'])\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Classification Report:\\n{class_report}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4e02be5bf7b842819578df9d84c45070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_853e05b268624dbaa4f999dd51694cb5",
            "placeholder": "​",
            "style": "IPY_MODEL_9f6c7c461f014c7a8eb387143897dc70",
            "value": "(…)-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf: 100%"
          }
        },
        "853e05b268624dbaa4f999dd51694cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f6c7c461f014c7a8eb387143897dc70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b081098f166a4daa8eee0cca71597e10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd3c9db1f7b34848ba32171947ee6681": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3b7e4b99cad4a00924200df2d35256c",
            "max": 4920733728,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9467fbef73741599d3148a8eb856403",
            "value": 4920733728
          }
        },
        "c9467fbef73741599d3148a8eb856403": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3b7e4b99cad4a00924200df2d35256c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7954c5315d2426796aa1058712af85b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e02be5bf7b842819578df9d84c45070",
              "IPY_MODEL_bd3c9db1f7b34848ba32171947ee6681",
              "IPY_MODEL_f099d344f53347d39ab2dbfaf300b380"
            ],
            "layout": "IPY_MODEL_b081098f166a4daa8eee0cca71597e10"
          }
        },
        "f099d344f53347d39ab2dbfaf300b380": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f90ce1e8169d47f495ae0d225f1db003",
            "placeholder": "​",
            "style": "IPY_MODEL_f8c5e7e8b50942a5aff2dff4d8d1c7d5",
            "value": " 4.92G/4.92G [03:37&lt;00:00, 22.4MB/s]"
          }
        },
        "f8c5e7e8b50942a5aff2dff4d8d1c7d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f90ce1e8169d47f495ae0d225f1db003": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}