{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models for  Movie Reviews Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>With a top-notch cast and dazzling special eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rotten</td>\n",
       "      <td>What's really lacking in The Lightning Thief i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_type                                     review_content\n",
       "0       Fresh  A fantasy adventure that fuses Greek mythology...\n",
       "1       Fresh  Uma Thurman as Medusa, the gorgon with a coiff...\n",
       "2       Fresh  With a top-notch cast and dazzling special eff...\n",
       "3       Fresh  Whether audiences will get behind The Lightnin...\n",
       "4      Rotten  What's really lacking in The Lightning Thief i..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# load the training dataset\n",
    "df = pd.read_csv('rotten_tomatoes_critic_reviews.csv', delimiter=',', header='infer')\n",
    "df.columns\n",
    "data=df[['review_type','review_content']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "review_type           0\n",
      "review_content    65806\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "review_type       0\n",
      "review_content    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataCleaned= data.dropna()\n",
    "dataCleaned.head()\n",
    "missing_values = dataCleaned.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_type\n",
       "Fresh     681035\n",
       "Rotten    383176\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "dataCleaned.groupby('review_type').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)\n",
    "dataCleaned['review_type'].value_counts()                                                                                                                                                                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will remove stopwords, punctuations\n",
    "# as well as we will apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_type\n",
      "Fresh     10000\n",
      "Rotten    10000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "target_level_1_data = dataCleaned[dataCleaned['review_type'] == 'Fresh']\n",
    "target_level_2_data = dataCleaned[dataCleaned['review_type'] == 'Rotten']\n",
    "\n",
    "sampled_data_level_1 = target_level_1_data.sample(n=10000, random_state=42)\n",
    "sampled_data_level_2 = target_level_2_data.sample(n=10000, random_state=42)\n",
    "\n",
    "sampled_data = pd.concat([sampled_data_level_1, sampled_data_level_2])\n",
    "\n",
    "# Reset index if needed\n",
    "sampled_data.reset_index(drop=True, inplace=True)\n",
    "print(sampled_data['review_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sampled_data['review_content']\n",
    "y = sampled_data['review_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "influential_stop_words = {\n",
    "    'only', 'alone', 'but', 'except', 'however', 'although', 'though', 'neither', 'nevertheless',\n",
    "    'either', 'yet', 'not', 'no', 'none', 'nobody', 'nothing', 'nowhere', 'nor', 'without', 'except'\n",
    "}\n",
    "\n",
    "punct = string.punctuation\n",
    "stopwords = list(STOP_WORDS - influential_stop_words)\n",
    "\n",
    "\n",
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    tokens = [] # list of tokens\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=text_data_cleaning)\n",
    "# tokenizer=text_data_cleaning, tokenization will be done according to this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training finetuning and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for svm: {'clf__C': 10, 'clf__kernel': 'rbf'}\n",
      "Train accuracy for svm: 0.9998\n",
      "Test accuracy for svm: 0.7460\n",
      "Classification report for svm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Fresh       0.75      0.75      0.75      2000\n",
      "      Rotten       0.75      0.74      0.75      2000\n",
      "\n",
      "    accuracy                           0.75      4000\n",
      "   macro avg       0.75      0.75      0.75      4000\n",
      "weighted avg       0.75      0.75      0.75      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for naive_bayes: {'clf__alpha': 1.0}\n",
      "Train accuracy for naive_bayes: 0.8926\n",
      "Test accuracy for naive_bayes: 0.7425\n",
      "Classification report for naive_bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Fresh       0.73      0.77      0.75      2000\n",
      "      Rotten       0.75      0.72      0.74      2000\n",
      "\n",
      "    accuracy                           0.74      4000\n",
      "   macro avg       0.74      0.74      0.74      4000\n",
      "weighted avg       0.74      0.74      0.74      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for random_forest: {'clf__max_depth': None, 'clf__n_estimators': 200}\n",
      "Train accuracy for random_forest: 0.9998\n",
      "Test accuracy for random_forest: 0.7205\n",
      "Classification report for random_forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Fresh       0.74      0.68      0.71      2000\n",
      "      Rotten       0.70      0.76      0.73      2000\n",
      "\n",
      "    accuracy                           0.72      4000\n",
      "   macro avg       0.72      0.72      0.72      4000\n",
      "weighted avg       0.72      0.72      0.72      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\DINA\\anaconda3\\envs\\DM_ENV\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for gradient_boosting: {'clf__learning_rate': 1.0, 'clf__n_estimators': 200}\n",
      "Train accuracy for gradient_boosting: 0.8248\n",
      "Test accuracy for gradient_boosting: 0.6850\n",
      "Classification report for gradient_boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Fresh       0.70      0.65      0.67      2000\n",
      "      Rotten       0.67      0.72      0.69      2000\n",
      "\n",
      "    accuracy                           0.69      4000\n",
      "   macro avg       0.69      0.69      0.68      4000\n",
      "weighted avg       0.69      0.69      0.68      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming tfidf is already defined\n",
    "# tfidf = TfidfVectorizer()\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'svm': SVC(),\n",
    "    'naive_bayes': MultinomialNB(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'gradient_boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Define the pipelines\n",
    "pipelines = {\n",
    "    'svm': Pipeline([('tfidf', tfidf), ('clf', SVC())]),\n",
    "    'naive_bayes': Pipeline([('tfidf', tfidf), ('clf', MultinomialNB())]),\n",
    "    'random_forest': Pipeline([('tfidf', tfidf), ('clf', RandomForestClassifier())]),\n",
    "    'gradient_boosting': Pipeline([('tfidf', tfidf), ('clf', GradientBoostingClassifier())])\n",
    "}\n",
    "\n",
    "# Define the parameter grids\n",
    "param_grids = {\n",
    "    'svm': {\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'naive_bayes': {\n",
    "        'clf__alpha': [0.1, 0.5, 1.0]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'clf__n_estimators': [100, 200],\n",
    "        'clf__max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'clf__n_estimators': [100, 200],\n",
    "        'clf__learning_rate': [0.01, 0.1, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Split the data with stratify\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train and evaluate the models\n",
    "for model_name in models.keys():\n",
    "    pipeline = pipelines[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=1)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    y_train_pred = grid_search.predict(x_train)\n",
    "    y_test_pred = grid_search.predict(x_test)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Train accuracy for {model_name}: {train_accuracy:.4f}\")\n",
    "    print(f\"Test accuracy for {model_name}: {test_accuracy:.4f}\")\n",
    "    print(f\"Classification report for {model_name}:\\n{classification_report(y_test, y_test_pred)}\")\n",
    "    \n",
    "    # Save the best model after fine-tuning\n",
    "    with open(f\"{model_name}_tfidf.pickle\", 'wb') as f:\n",
    "        pickle.dump(grid_search.best_estimator_, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained Sentence Transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Split the data with stratify\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Encode the data\n",
    "x_train_enc = model.encode(x_train.tolist(), show_progress_bar=False)\n",
    "x_test_enc = model.encode(x_test.tolist(), show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for svm: {'C': 1, 'kernel': 'rbf'}\n",
      "Train accuracy for svm: 0.8694\n",
      "Test accuracy for svm: 0.7778\n",
      "Classification report for svm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Fresh       0.79      0.76      0.77      2000\n",
      "      Rotten       0.77      0.80      0.78      2000\n",
      "\n",
      "    accuracy                           0.78      4000\n",
      "   macro avg       0.78      0.78      0.78      4000\n",
      "weighted avg       0.78      0.78      0.78      4000\n",
      "\n",
      "Best parameters for naive_bayes: {'alpha': 0.1}\n",
      "Train accuracy for naive_bayes: 0.7260\n",
      "Test accuracy for naive_bayes: 0.7462\n",
      "Classification report for naive_bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Fresh       0.75      0.73      0.74      2000\n",
      "      Rotten       0.74      0.76      0.75      2000\n",
      "\n",
      "    accuracy                           0.75      4000\n",
      "   macro avg       0.75      0.75      0.75      4000\n",
      "weighted avg       0.75      0.75      0.75      4000\n",
      "\n",
      "Best parameters for random_forest: {'max_depth': None, 'n_estimators': 200}\n",
      "Train accuracy for random_forest: 0.9998\n",
      "Test accuracy for random_forest: 0.7445\n",
      "Classification report for random_forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Fresh       0.75      0.74      0.74      2000\n",
      "      Rotten       0.74      0.75      0.75      2000\n",
      "\n",
      "    accuracy                           0.74      4000\n",
      "   macro avg       0.74      0.74      0.74      4000\n",
      "weighted avg       0.74      0.74      0.74      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Transform embeddings to non-negative values for MultinomialNB\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_enc_nb = scaler.fit_transform(x_train_enc)\n",
    "x_test_enc_nb = scaler.transform(x_test_enc)\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'svm': SVC(probability=True),  # Enable probability estimation\n",
    "    'naive_bayes': MultinomialNB(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'gradient_boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'svm': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'naive_bayes': {\n",
    "        'alpha': [0.1, 0.5, 1.0]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and evaluate the models\n",
    "for model_name, model in models.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=1)\n",
    "    \n",
    "    if model_name == 'naive_bayes':\n",
    "        grid_search.fit(x_train_enc_nb, y_train)\n",
    "    else:\n",
    "        grid_search.fit(x_train_enc, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    if model_name == 'naive_bayes':\n",
    "        y_train_pred = grid_search.predict(x_train_enc_nb)\n",
    "        y_test_pred = grid_search.predict(x_test_enc_nb)\n",
    "    else:\n",
    "        y_train_pred = grid_search.predict(x_train_enc)\n",
    "        y_test_pred = grid_search.predict(x_test_enc)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Train accuracy for {model_name}: {train_accuracy:.4f}\")\n",
    "    print(f\"Test accuracy for {model_name}: {test_accuracy:.4f}\")\n",
    "    print(f\"Classification report for {model_name}:\\n{classification_report(y_test, y_test_pred)}\")\n",
    "    \n",
    "    # Save the best model after fine-tuning\n",
    "    with open(f\"{model_name}_sentence_transformers.pickle\", 'wb') as f:\n",
    "        pickle.dump(grid_search.best_estimator_, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# List of reviews to predict\n",
    "reviews = [\n",
    "    \"The product was excellent and exceeded my expectations.\",\n",
    "    \"Worst experience ever. Will not buy again.\",\n",
    "    \"It was okay, not great but not bad either.\",\n",
    "    \"I love it! Will definitely recommend to others.\"\n",
    "]\n",
    "\n",
    "# Loop through each model and its vectorizer, load them, and make predictions\n",
    "for model_filename, vectorizer_filename in zip(model_filenames, vectorizer_filenames):\n",
    "    with open(model_filename, 'rb') as model_file, open(vectorizer_filename, 'rb') as vectorizer_file:\n",
    "        model = pickle.load(model_file)\n",
    "        vectorizer = pickle.load(vectorizer_file)\n",
    "    \n",
    "    # Encode the reviews using the vectorizer\n",
    "    X_reviews = vectorizer.transform(reviews)\n",
    "    \n",
    "    # Predict the class of the reviews\n",
    "    predictions = model.predict(X_reviews)\n",
    "    \n",
    "    # Print the predictions\n",
    "    print(f'Predictions using {model_filename}:')\n",
    "    for review, prediction in zip(reviews, predictions):\n",
    "        print(f\"Review: {review}\\nPrediction: {prediction}\\n\")\n",
    "    print('\\n' + '='*50 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using svm_sentence_transformers.pickle:\n",
      "Review: An absolute masterpiece! The Shawshank Redemption is a tale of hope, friendship, and resilience. Tim Robbins and Morgan Freeman deliver stellar performances, and the story is deeply moving. The films message about the power of hope is timeless and profound. A must-watch!\n",
      "Prediction: Fresh\n",
      "\n",
      "Review: Despite the hype, Batman v Superman was a disappointment. The plot was convoluted, and the pacing was all over the place. While Ben Affleck and Henry Cavill gave decent performances, the film felt like it was trying to do too much at once without succeeding in any of its goals. The overuse of CGI didnt help either.\n",
      "Prediction: Rotten\n",
      "\n",
      "Review: Cats was an unfortunate misstep. The visual effects were unsettling, and the plot was nearly nonexistent. Even with a star-studded cast, the film couldnt overcome its many flaws. It felt more like a surreal fever dream than a cohesive story. Its hard to recommend this to anyone, even die-hard fans of the original musical.\n",
      "Prediction: Rotten\n",
      "\n",
      "Review: nception is a mind-bending journey through the world of dreams. Christopher Nolans direction is brilliant, and the visual effects are stunning. Leonardo DiCaprio leads a fantastic cast, and the plot keeps you on the edge of your seat. Its an intellectual and thrilling experience that stays with you long after it ends.\n",
      "Prediction: Fresh\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions using random_forest_sentence_transformers.pickle:\n",
      "Review: An absolute masterpiece! The Shawshank Redemption is a tale of hope, friendship, and resilience. Tim Robbins and Morgan Freeman deliver stellar performances, and the story is deeply moving. The films message about the power of hope is timeless and profound. A must-watch!\n",
      "Prediction: Fresh\n",
      "\n",
      "Review: Despite the hype, Batman v Superman was a disappointment. The plot was convoluted, and the pacing was all over the place. While Ben Affleck and Henry Cavill gave decent performances, the film felt like it was trying to do too much at once without succeeding in any of its goals. The overuse of CGI didnt help either.\n",
      "Prediction: Rotten\n",
      "\n",
      "Review: Cats was an unfortunate misstep. The visual effects were unsettling, and the plot was nearly nonexistent. Even with a star-studded cast, the film couldnt overcome its many flaws. It felt more like a surreal fever dream than a cohesive story. Its hard to recommend this to anyone, even die-hard fans of the original musical.\n",
      "Prediction: Rotten\n",
      "\n",
      "Review: nception is a mind-bending journey through the world of dreams. Christopher Nolans direction is brilliant, and the visual effects are stunning. Leonardo DiCaprio leads a fantastic cast, and the plot keeps you on the edge of your seat. Its an intellectual and thrilling experience that stays with you long after it ends.\n",
      "Prediction: Fresh\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions using naive_bayes_sentence_transformers.pickle:\n",
      "Review: An absolute masterpiece! The Shawshank Redemption is a tale of hope, friendship, and resilience. Tim Robbins and Morgan Freeman deliver stellar performances, and the story is deeply moving. The films message about the power of hope is timeless and profound. A must-watch!\n",
      "Prediction: Fresh\n",
      "\n",
      "Review: Despite the hype, Batman v Superman was a disappointment. The plot was convoluted, and the pacing was all over the place. While Ben Affleck and Henry Cavill gave decent performances, the film felt like it was trying to do too much at once without succeeding in any of its goals. The overuse of CGI didnt help either.\n",
      "Prediction: Rotten\n",
      "\n",
      "Review: Cats was an unfortunate misstep. The visual effects were unsettling, and the plot was nearly nonexistent. Even with a star-studded cast, the film couldnt overcome its many flaws. It felt more like a surreal fever dream than a cohesive story. Its hard to recommend this to anyone, even die-hard fans of the original musical.\n",
      "Prediction: Rotten\n",
      "\n",
      "Review: nception is a mind-bending journey through the world of dreams. Christopher Nolans direction is brilliant, and the visual effects are stunning. Leonardo DiCaprio leads a fantastic cast, and the plot keeps you on the edge of your seat. Its an intellectual and thrilling experience that stays with you long after it ends.\n",
      "Prediction: Fresh\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# List of model filenames\n",
    "model_filenames = ['svm_sentence_transformers.pickle', 'random_forest_sentence_transformers.pickle', 'naive_bayes_sentence_transformers.pickle']  # Add more model filenames as needed\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "# List of reviews to predict\n",
    "reviews = [\n",
    "    'An absolute masterpiece! The Shawshank Redemption is a tale of hope, friendship, and resilience. Tim Robbins and Morgan Freeman deliver stellar performances, and the story is deeply moving. The films message about the power of hope is timeless and profound. A must-watch!',\n",
    "    'Despite the hype, Batman v Superman was a disappointment. The plot was convoluted, and the pacing was all over the place. While Ben Affleck and Henry Cavill gave decent performances, the film felt like it was trying to do too much at once without succeeding in any of its goals. The overuse of CGI didnt help either.',\n",
    "    \"Cats was an unfortunate misstep. The visual effects were unsettling, and the plot was nearly nonexistent. Even with a star-studded cast, the film couldnt overcome its many flaws. It felt more like a surreal fever dream than a cohesive story. Its hard to recommend this to anyone, even die-hard fans of the original musical.\",\n",
    "    \"nception is a mind-bending journey through the world of dreams. Christopher Nolans direction is brilliant, and the visual effects are stunning. Leonardo DiCaprio leads a fantastic cast, and the plot keeps you on the edge of your seat. Its an intellectual and thrilling experience that stays with you long after it ends.\"\n",
    "]\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "\n",
    "# Encode the reviews using the Sentence Transformer\n",
    "X_reviews = model.encode(reviews)\n",
    "\n",
    "# Loop through each model, load it, and make predictions\n",
    "for model_filename in model_filenames:\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "    \n",
    "    # Predict the class of the reviews\n",
    "    predictions = model.predict(X_reviews)\n",
    "    \n",
    "    # Print the predictions\n",
    "    print(f'Predictions using {model_filename}:')\n",
    "    for review, prediction in zip(reviews, predictions):\n",
    "        print(f\"Review: {review}\\nPrediction: {prediction}\\n\")\n",
    "    print('\\n' + '='*50 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
